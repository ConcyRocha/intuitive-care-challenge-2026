# Teste de Nivelamento - Est√°gio Intuitive Care 
**Linguagem Utilizada: Python**


Este reposit√≥rio cont√©m a solu√ß√£o do teste pr√°tico para a vaga de est√°gio na **Intuitive Care**. O objetivo deste projeto foi ir al√©m do funcionamento b√°sico: apliquei conceitos de Engenharia de Dados para criar automa√ß√£o robusta, escal√°vel e documentada, pronta para lidar com cen√°rios reais de varia√ß√£o de dados

### üìê Arquitetura da Solu√ß√£o

Fluxo de dados 

```mermaid
graph TD
    %% N√≥s do Grafo
    ANS[Portal de Dados Abertos ANS] -->|Scraping| S1[Stage 1.1: Coleta]
    S1 -->|ZIP Files| RAW[(Data Raw)]
    
    RAW -->|Extra√ß√£o Incremental| S2[Stage 1.2: Processamento]
    
    subgraph ETL [Pipeline de Transforma√ß√£o]
        S2 -->|Detecta Formato| CLEAN{Limpeza}
        CLEAN -->|UTF-8/Latin1| NORM[Normaliza√ß√£o de Schema]
        NORM -->|Float Conversion| CONSOL[Consolida√ß√£o]
    end
    
    CONSOL -->|CSV √önico| PROCESSED[(Data Processed)]
    
    PROCESSED -->|Chunk Load| S3[Stage 2: Carga SQL]
    S3 -->|Insert| DB[(Banco de Dados SQL)]

    %% Estiliza√ß√£o (Opcional, deixa colorido)
    style ANS fill:#f9f,stroke:#333,stroke-width:2px
    style DB fill:#bbf,stroke:#333,stroke-width:2px
    style ETL fill:#e1f5fe,stroke:#01579b,stroke-dasharray: 5 5

```


## 1. Prepara√ß√£o do Ambiente

Para garantir a reprodutibilidade do teste e manter o sistema global organizado, a solu√ß√£o utiliza um ambiente virtual isolado.

### Pr√©-requisitos
* **Python 3.10+**
* **Bibliotecas base:** `requests`, `beautifulsoup4`, `pandas`

### Configura√ß√£o Passo a Passo
1. **Cria√ß√£o do Ambiente Virtual (venv):**
   Utilizei o terminal integrado do VS Code para criar um ambiente isolado:
   ```bash
   python -m venv venv
### Ativa√ß√£o do Ambiente

* **Windows:**
  ```bash
  .\venv\Scripts\activate

* **macOS/Linux:**
```bash
source .venv/bin/activate
```

---

## Etapa 1.1: Integra√ß√£o com API P√∫blica da ANS

O objetivo desta etapa √© acessar a API de Dados Abertos da ANS e realizar o download das **Demonstra√ß√µes Cont√°beis** dos √∫ltimos 3 trimestres dispon√≠veis.

### Execu√ß√£o do Script
O script de coleta est√° localizado na pasta `backend`:
```bash
python backend/stage_1_api.py
```
### Decis√µes T√©cnicas e Trade-offs

* **Mapeamento Din√¢mico de URLs:**
    * **Decis√£o:** O c√≥digo utiliza a biblioteca `BeautifulSoup` para navegar nas tags HTML do servidor da ANS, identificando pastas no formato `YYYY/QQ/`.
    * **Justificativa:** A estrutura de diret√≥rios pode mudar ao longo do tempo. Esta abordagem torna o script adapt√°vel a varia√ß√µes de nomenclatura e garante a captura autom√°tica dos dados mais recentes sem interven√ß√£o manual.

* **Persist√™ncia Incremental (Staging Area):**
    * **Decis√£o:** Armazenar os arquivos ZIP originais em uma pasta local (`data/raw`) antes de iniciar o processamento.
    * **Justificativa:** Dado o volume de dados e a instabilidade potencial de APIs p√∫blicas, ter os dados brutos salvos permite repetir etapas de extra√ß√£o e limpeza sem a necessidade de novos downloads, economizando tempo e banda de rede.

* **Qualidade de C√≥digo (PEP 8):**
    * **Decis√£o:** Refatora√ß√£o para manter limites de caracteres e espa√ßamento padr√£o.
    * **Justificativa:** Focar na legibilidade e manutenibilidade do c√≥digo, facilitando a revis√£o t√©cnica.

### Resultados da Execu√ß√£o (Etapa 1.1)

A execu√ß√£o do script `stage_1_api.py` realizou a varredura recursiva no servidor da ANS e identificou os trimestres mais recentes.

**1. Log de Execu√ß√£o:**
![Log do Terminal com Sucesso](assets/image1.png)
*Figura 1: Terminal exibindo a identifica√ß√£o dos anos e o download dos arquivos ZIP.*

**2. Persist√™ncia dos Dados:**
![Arquivos Baixados](assets/image2.png)
*Figura 2: Verifica√ß√£o da pasta `data/raw` contendo os arquivos `1T2025.zip`, `2T2025.zip` e `3T2025.zip`.*

## Etapa 1.2: Pipeline de Transforma√ß√£o e Limpeza (ETL)

Esta etapa √© respons√°vel por processar os arquivos brutos (ZIPs), normalizar as discrep√¢ncias e consolidar os dados para an√°lise.

### 1.2.1 Decis√£o de Arquitetura: Processamento Incremental
Para garantir performance e estabilidade, optei por uma abordagem de **Batch Processing Incremental** ao inv√©s de carregar todos os dados em mem√≥ria (*In-Memory*).

* **Implementa√ß√£o:** O script processa um arquivo ZIP por vez (extra√ß√£o -> transforma√ß√£o -> carga -> limpeza tempor√°ria).
* **Justificativa (Trade-off):** Optei por processar os dados aos poucos (incrementalmente) em vez de carregar tudo de uma vez, garantimos a estabilidade do sistema. Essa abordagem impede que a mem√≥ria acabe (erro de mem√≥ria cheia), permitindo que o script processe volumes gigantescos de dados sem falhar, mesmo em m√°quinas com pouca pot√™ncia.
### 1.2.2 Estrat√©gia de Normaliza√ß√£o (Data Wrangling)
Para atender ao desafio de **variedade de formatos** (CSV, TXT, colunas inconsistentes) e **evolu√ß√£o de schema**, implementei uma camada de adapta√ß√£o sem√¢ntica:

* **Ingest√£o Polim√≥rfica:** O pipeline detecta automaticamente a extens√£o e aplica estrat√©gias de *fallback* para diferentes encodings (`latin1` vs `utf-8`) e separadores (`;` vs `,`), garantindo a leitura correta tanto de arquivos legados quanto modernos.
* **Mapeamento Can√¥nico (`Schema Mapping`):** Utiliza√ß√£o de um dicion√°rio de tradu√ß√£o para unificar nomenclaturas variadas da ANS.
    * *Exemplo:* As colunas `DT_REGISTRO`, `DATA` e `ANO_TRIMESTRE` s√£o todas normalizadas para o campo √∫nico `data_referencia`.
    * **Resili√™ncia:** Colunas essenciais ausentes nos arquivos mais antigos s√£o geradas com valores nulos (`None`), mantendo a integridade da estrutura final.
* **Sanitiza√ß√£o de Tipos:** Convers√£o robusta de valores monet√°rios no formato brasileiro (ex: `"1.000,00"`) para floats comput√°veis (`1000.0`).

### 1.2.3 Resultados da Execu√ß√£o
O pipeline foi capaz de processar e unificar os dados dos 3 trimestres com sucesso.

* **Volume Processado:** **2.113.924 registros** consolidados.
* **Arquivo Final:** `data/processed/despesas_consolidadas.csv`

**Evid√™ncia de Performance:**
![Sucesso no ETL](assets/image4.png)
*Figura 3: Log de execu√ß√£o comprovando o processamento de mais de 2 milh√µes de linhas com a estrat√©gia incremental.*

**Amostra dos Dados Consolidados:**
![Preview do Arquivo CSV](assets/image5.png)
*Figura 4: Visualiza√ß√£o da estrutura do arquivo `despesas_consolidadas.csv`, demonstrando a unifica√ß√£o das colunas e a identifica√ß√£o da origem dos dados.*

## Etapa 1.3. Consolida√ß√£o e An√°lise de Inconsist√™ncias

Esta etapa realiza o enriquecimento dos dados atrav√©s do cruzamento com a base cadastral `Relatorio_cadop.csv`. **Este passo foi fundamental pois os arquivos originais das demonstra√ß√µes cont√°beis n√£o cont√™m o CNPJ (apenas o Registro ANS)**. Al√©m disso, o script aplica limpeza e padroniza√ß√£o de dados.

**Comando para execu√ß√£o:**
```bash
python backend/stage_1_3_analysis.py
```
### 1.3.1 An√°lise Cr√≠tica e Tratamento de Dados (Data Quality)

Durante o processo de consolida√ß√£o, foram identificadas inconsist√™ncias nativas dos dados da ANS. Abaixo, detalho as tratativas aplicadas e suas justificativas t√©cnicas:

| Inconsist√™ncia Identificada | Tratativa Aplicada | Justificativa da Abordagem |
| :--- | :--- | :--- |
| **CNPJs Duplicados** | **Normaliza√ß√£o:** Criado um mapa de `1 CNPJ -> 1 Raz√£o Social` (baseado no registro mais recente/dispon√≠vel). | Empresas alteram a raz√£o social, mas mant√™m o CNPJ. A normaliza√ß√£o √© obrigat√≥ria para evitar a quebra de linhas em agrupamentos (Group By). |
| **Valores Zerados** | **Remo√ß√£o:** Linhas com `vl_saldo_final == 0` foram exclu√≠das. | No plano de contas, operadoras enviam a estrutura completa, mesmo sem movimenta√ß√£o. Manter zeros apenas infla o armazenamento sem agregar valor. |
| **Valores Negativos** | **Mantidos:** Valores menores que zero foram preservados. | Contabilmente, despesas negativas representam estornos, glosas ou ajustes de cr√©dito. Remover esses dados geraria um saldo final incorreto. |
| **Formatos de Data** | **Padroniza√ß√£o via Regex:** Extra√ß√£o direta dos d√≠gitos de Ano e Trimestre do nome do arquivo. | Ignoramos a formata√ß√£o textual (que variava entre `1T2025`, `2025_01`) e for√ßamos a tipagem para Inteiro (`Int64`), facilitando ordena√ß√£o. |

### 1.3.2 Resultados da Execu√ß√£o

**1. Log de Execu√ß√£o e Enriquecimento:**
O script contornou as prote√ß√µes de download da ANS (usando User-Agent), detectou automaticamente o encoding correto e realizou a limpeza dos nomes das colunas (removendo caracteres ocultos/BOM) para garantir o merge perfeito:

![Log de Execu√ß√£o](assets/image6.png)
*Figura 5: Visualiza√ß√£o da saida esperada com o zip `despesas_consolidadas.zip` criado.

## Etapa 2 Teste de transforma√ß√£o de dados

### Execu√ß√£o do Script
O script de coleta est√° localizado na pasta `backend`:
```bash
python backend/stage_2_1_validation.py
```
## 2.1 Valida√ß√£o de dados com estrat√©gias diferentes

O script aplica tr√™s regras de neg√≥cio rigorosas sobre o dataset consolidado:

* **CNPJ:** Valida√ß√£o matem√°tica dos d√≠gitos verificadores (algoritmo M√≥dulo 11 da Receita Federal), e n√£o apenas valida√ß√£o de formato/m√°scara.
* **Financeiro:** Filtro estrito para valores positivos (`> 0`). Valores zerados ou negativos s√£o segregados.
* **Completude:** Rejei√ß√£o de registros sem Raz√£o Social identificada.

### Trade-off T√©cnico: Tratamento de CNPJs Inv√°lidos

**Estrat√©gia Escolhida: Segrega√ß√£o (Pattern: Valid & Invalid Sinks)**
Optei por separar o fluxo de dados em dois destinos: um arquivo para dados confi√°veis e outro para dados rejeitados.

**Justificativa da Decis√£o:**
Ao direcionar as falhas para um arquivo `despesas_rejeitadas.csv` contendo o motivo do erro, garantimos:

1.  **Rastreabilidade:** Nenhuma informa√ß√£o fiscal √© perdida.
2.  **Continuidade:** O pipeline n√£o para por causa de dados ruins.
3.  **Auditoria:** O arquivo de rejeitados serve como insumo para que a equipe de neg√≥cios ou TI corrija os dados na fonte.

### Resultados da Execu√ß√£o

**1. Log de Valida√ß√£o (Terminal):**
O script processou mais de 600 mil registros. Note que cerca de **70.000 registros foram rejeitados** (a maioria devido √† regra estrita de valores positivos/zerados solicitada no teste e valida√ß√£o matem√°tica de CNPJ), demonstrando a efic√°cia do filtro.

![Log de Valida√ß√£o](assets/image8.png)
*Figura 6: Terminal exibindo o resumo estat√≠stico da valida√ß√£o e a contagem de registros rejeitados.*

**2. Segrega√ß√£o dos Arquivos (Sink):**
Como resultado, o pipeline gerou dois arquivos distintos na pasta `processed`:

* `despesas_validas.csv`: Dados limpos e prontos para uso.
* `despesas_rejeitadas.csv`: Dados impuros para an√°lise de causa raiz.

![Arquivos Separados](assets/image9.png)

*Figura 7: Visualiza√ß√£o da pasta `processed` mostrando a aplica√ß√£o do padr√£o de separa√ß√£o.

## 2.2. Enriquecimento de Dados com Tratamento de Falhas

Esta etapa finaliza a prepara√ß√£o dos dados adicionando contexto geogr√°fico (`UF`) e categoriza√ß√£o de neg√≥cio (`Modalidade`) ao dataset validado.

**Comando para execu√ß√£o:**
```bash
python backend/stage_2_2_enrichment.py
````
### Estrat√©gia de Processamento e Join

Para cruzar as despesas com o cadastro das operadoras, utilizei as seguintes estrat√©gias t√©cnicas:

| Desafio | Solu√ß√£o Adotada | Justificativa |
| :--- | :--- | :--- |
| **Arquitetura de Processamento** | **Pandas In-Memory** | O volume total (~600k linhas) cabe confortavelmente na mem√≥ria RAM (consumo estimado < 200MB). Ferramentas distribu√≠das (Spark) adicionariam complexidade desnecess√°ria para este volume ("Small Data"). |
| **Registros sem Match** | **Left Join** | Priorizamos as despesas. Se uma operadora tem despesas mas n√£o est√° no cadastro ativo (ex: faliu ou mudou de status), mantemos o registro financeiro e preenchemos a UF como "N√£o Informado". O Inner Join causaria perda de dados cont√°beis. |
| **Duplicidade no Cadastro** | **Deduplica√ß√£o Pr√©via** | O cadastro da ANS pode conter hist√≥rico. Antes do join, aplicamos `drop_duplicates(subset='CNPJ')` para garantir uma rela√ß√£o 1:1. Isso impede a "explos√£o de linhas" (Cartesian Product) no resultado final. |

### üì¶ Resultado Final

O arquivo `data/processed/dataset_final_enriquecido.csv` representa a **"Gold Layer"** deste pipeline: dados limpos, validados e enriquecidos, prontos para visualiza√ß√£o em Dashboards ou ingest√£o em Banco de Dados.

**1. Log de Execu√ß√£o (Enriquecimento):**
O script carrega as despesas v√°lidas, baixa o cadastro atualizado e realiza o cruzamento (Left Join):

![Log de Enriquecimento](assets/image10.png)
*Figura 10: Log do terminal demonstrando o download do cadastro, a deduplica√ß√£o de CNPJs e o sucesso do Left Join.*

**2. Dataset Final (Gold Layer):**
Amostra do arquivo final demonstrando as novas colunas (`Modalidade`, `RegistroANS`, `UF`) integradas corretamente ao dataset financeiro:

![CSV Final Enriquecido](assets/image11.png)
*Figura 11: Amostra dos dados enriquecido com as colunas UF, Modalidade e RegistroANS.*

## 2.3. Agrega√ß√£o Anal√≠tica e Entrega Final

Nesta  etapa, foi transformados os dados transacionais em informa√ß√µes gerenciais, agrupando despesas por Operadora e UF para identificar os maiores players e sua estabilidade financeira.

**Comando para execu√ß√£o:**
```bash
python backend/stage_2_3_aggregation.py
````

###  M√©tricas Calculadas

Para cada Operadora/UF, calculamos:

* **Total de Despesas:** Volume financeiro total no per√≠odo.
* **M√©dia Trimestral:** Valor m√©dio dos lan√ßamentos.
* **Desvio Padr√£o:** Mede a volatilidade. Operadoras com desvio alto t√™m gastos muito irregulares; desvio baixo indica custos constantes.

### Trade-off T√©cnico: Estrat√©gia de Ordena√ß√£o

Para cumprir o requisito de ordenar os dados pelo "Valor Total" (do maior para o menor), foi necess√°rio escolher um algoritmo de ordena√ß√£o (Sorting).

| Estrat√©gia | Complexidade | Decis√£o |
| :--- | :--- | :--- |
| **In-Memory Sort (TimSort/QuickSort)** | **O(N log N)** | **[ESCOLHIDA]** O Pandas utiliza o *TimSort* (derivado do Merge Sort e Insertion Sort) por padr√£o. Como o dados agregado resultou em menos de 10.000 linhas (ap√≥s o Group By), essa opera√ß√£o √© instant√¢nea (< 0.1s) e altamente eficiente em CPU. |
| **External Merge Sort** | O(N log N) (I/O Bound) | **Descartada.** Seria necess√°ria apenas se o volume de dados agregados excedesse a mem√≥ria RAM (ex: bilh√µes de linhas), o que n√£o √© o caso deste teste. |
| **Database Indexing** | O(N) (Se indexado) | **Descartada.** Carregar os dados num banco SQL apenas para ordenar adicionaria lat√™ncia de rede e complexidade de infraestrutura desnecess√°ria para um script ETL standalone. |

###  Artefato Final (Entrega)

O script gera automaticamente o arquivo compactado conforme solicitado nas instru√ß√µes do teste:

* **Arquivo:** `Teste_ConceicaoRocha.zip`
* **Conte√∫do:** `despesas_agregadas.csv` (Ordenado e consolidado).
* **Localiza√ß√£o:** Raiz do projeto.

## 3. Teste de Banco de Dados e An√°lise SQL

Nesta etapa, foi estruturado um banco de dados relacional para armazenar os dados processados e executar an√°lises.

### 3.1 & 3.2. Modelagem de Dados (DDL)

Os scripts de cria√ß√£o de tabelas est√£o dispon√≠veis em `sql/1_schema_ddl.sql`.

**Decis√µes de Arquitetura (Trade-offs):**

* **Normaliza√ß√£o (Op√ß√£o B - Escolhida):**
    * Adotamos o modelo **Star Schema** simplificado, separando `fact_despesas` (Fatos) e `dim_operadoras` (Dimens√£o).
    * *Motivo:* Reduz redund√¢ncia de armazenamento (o endere√ßo da operadora n√£o se repete milh√µes de vezes) e facilita a atualiza√ß√£o cadastral sem travar a tabela de despesas (fatos).
* **Tipos de Dados:**
    * **Valores Monet√°rios:** Utilizamos `DECIMAL(15,2)` ao inv√©s de `FLOAT`. *Justificativa:* Sistemas financeiros exigem precis√£o exata; `FLOAT` pode gerar erros de arredondamento de centavos.
    * **Datas:** Utilizamos `DATE` (ISO 8601) para permitir fun√ß√µes temporais nativas do SQL.

### 3.3. Estrat√©gia de Importa√ß√£o e Tratamento (ETL)

Para a ingest√£o dos dados (Item 3.3), optou-se por um pipeline **Python (Pandas) + SQL Alchemy** em vez de comandos SQL brutos (`COPY`).

**An√°lise Cr√≠tica de Inconsist√™ncias:**
Durante a ingest√£o, o script `backend/stage_3_db_test.py` trata automaticamente:
1.  **Datas Inconsistentes:** Converte colunas separadas de "Ano/Trimestre" em datas v√°lidas (`YYYY-MM-01`).
2.  **Strings em Num√©ricos:** Remove caracteres de moeda e converte para `Float/Decimal` antes da inser√ß√£o.
3.  **Valores NULL:** Preenchimento de valores nulos em m√©tricas financeiras com `0.0` para n√£o quebrar agrega√ß√µes (`SUM/AVG`).

### 3.4. Resultados das Queries Anal√≠ticas

As queries desenvolvidas em `sql/2_queries_analytics.sql` foram executadas com sucesso. Abaixo as evid√™ncias:

**Query 1: Top 5 Operadoras com Maior Crescimento (%)**
Identifica operadoras que tiveram explos√£o de custos entre o primeiro e o √∫ltimo trimestre analisado.
![Resultado Query 1](assets/query1.png)

**Query 2: Distribui√ß√£o Geogr√°fica de Despesas**
Lista os estados com maior volume financeiro e a m√©dia de custo por lan√ßamento.
![Resultado Query 2](assets/query2.png)

**Query 3: Operadoras Acima da M√©dia de Mercado**
Filtra operadoras que gastaram mais que a m√©dia global em pelo menos 2 trimestres distintos.
![Resultado Query 3](assets/query3.png)